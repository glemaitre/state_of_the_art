\subsection{\ac{cadx}: Classification} \label{subsec:classification}

\subsubsection{Classifier}

Once the feature vector has been extracted and eventually the complexity reduced, it is possible to make a decision and classify this feature vector to belong to \ac{cap} or healthy tissue. Classification methods used in a \ac{cad} for \ac{cap} system to distinguish these two classes are summarized in \ac{tab}~\ref{tab:class}. A full review of classification methods used in pattern recognition can be found in~\cite{Bishop2006}.

\setenumerate{listparindent=\parindent,itemsep=10px}
\setlist{noitemsep}
\begin{enumerate}[leftmargin=*]

\item[$-$] \textbf{\textit{Rule-based methods:}} Lv et al.~\cite{Lv2009} make use of a decision stump classifier to distinguish \ac{cap} and healthy classes. Puech et al.~\cite{Puech2009} detect \ac{cap} by implementing a given set of rules using a score scheme in a medical decision making approach. The feature values are compared with a pre-defined threshold. Then, at each comparison, the final score is incremented, depending on the threshold and the final decision is taken depending of this final score.

\item[$-$] \textbf{\textit{Clustering methods:}} \acf{knn} is one of the simplest supervised machine learning classification methods. \ac{knn} was one of the methods used in~\cite{Niaf2011,Niaf2012} mainly to make a comparison with different machine learning techniques. Litjens et al. in~\cite{Litjens2012} used this method to roughly detect potential \ac{cap} voxels before performing a region-based classification.

 The $k$-means algorithm is an unsupervised clustering method in which the data is iteratively partitioned into $k$ clusters. This algorithm can also be used for ``on-line'' learning. In case that new data has to be incorporated, the initial centroid positions correspond to the results of a previous $k$-means training and is followed by the assignment-updating stage previously explained.
  Tiwari et al. in~\cite{Tiwari2007,Tiwari2009} used $k$-means with three clusters corresponding to \ac{cap}, healthy and non-prostate, respectively. $k$-means was applied iteratively and the voxels corresponding to the largest cluster were excluded under the assumption that it was assigned to ``non-prostate'' cluster. The algorithm stopped when the number of voxels in all remaining clusters was smaller than a given threshold.
 Tiwari et al. in~\cite{Tiwari2008} and Viswanath et al. in~\cite{Viswanath2008,Viswanath2008a} used $k$-means in a repetitive manner in order to be less sensitive to the centroids initialisation. Thus, $k$ clusters were generated $T$ times. The final assignment was performed by majority voting using a co-association matrix as proposed in~\cite{Fred2005}.

\item[$-$] \textbf{\textit{Linear model classifiers:}} \Acf{lda} can be used as a classification method in which the optimal linear separation between two classes is found by maximizing the interclass variance and minimizing the intraclass variance~\cite{Friedman1989}. \Ac{lda} has been used in~\cite{Antic2013,Chan2003,Litjens2014,Niaf2011,Niaf2012,Vos2012}.
  Logistic regression can be used to perform binary classification and can provide the probability of an observation to belong to a class. This has been used to create a linear probabilistic model in~\cite{Kelm2007,Puech2009}.

\item[$-$] \textbf{\textit{Non-linear model classifier:}} Viswanath et al. in~\cite{Viswanath2012} used \acf{qda} instead of \ac{lda}. Unlike in \ac{lda} in which one assumes that the class covariance matrix $\Sigma$ is identical for all the classes, in \ac{qda}, a covariance matrix $\Sigma_k$ specific to each class is computed.

\item[$-$] \textbf{\textit{Probabilistic classifier:}} The most commonly used classifier is the naive Bayes classifier which is a probabilistic classifier assuming independence between each feature dimension~\cite{Rish2001}. The Naive Bayes classifier has been used in~\cite{Giannini2013,Mazzetti2011,Niaf2011,Niaf2012}. The Normal distribution was adopted as the likelihood probability for that model.

\item[$-$] \textbf{\textit{Ensemble learning classifiers:}} AdaBoost is an adaptive method based on an ensemble learning method and was initially proposed by~\cite{Freund1997}. AdaBoost linearly combines several weak learners resulting into a final strong classifier. A weak learner is defined as a classification method performing slightly better than random classification. Random forest\footnote{Random forest implementation can be found at: \url{http://www.stat.berkeley.edu/~breiman/RandomForests/cc\_software.htm}} is a classification method which is based on creating an ensemble of decision trees and was introduced in~\cite{Breiman2001}. Probabilistic boosting-tree is another ensemble learning classifier which shares principles with AdaBoost but using them inside a decision tree~\cite{Tu2005}. 
Lopes et al.~\cite{Lopes2011} make use of the AdaBoost classifier to perform their classification while Litjens et al. in~\cite{Litjens2014} used the GentleBoost variant~\cite{Friedman1998} which provides a modification of the function affecting the weight at each weak classifier. The random forest classifier has been used in~\cite{Kelm2007,Litjens2014,Tiwari2012,Tiwari2013,Viswanath2009} whereas the probabilistic boosting-tree classifier in~\cite{Tiwari2009a,Tiwari2012,Tiwari2010,Viswanath2011}.

\item[$-$] \textbf{\textit{Kernel method:}} A Gaussian process\footnote{Gaussian process implementation can be found at: \url{http://www.gaussianprocess.org/gpml/code/matlab/doc/index.html}} for classification is a kernel method in which it is assumed that the data can be represented by a single sample from a multivariate Gaussian distribution~\cite{Rasmussen2005}. Only the work of Kelm et al.~\cite{Kelm2007} used a Gaussian process for classification in \ac{mrsi} data.

\item[$-$] \textbf{\textit{Sparse kernel methods:}} In a classification scheme using Gaussian processes, when a prediction has to be performed, the whole training data will be used to assign a label to the new observations. That is why this method is also called kernel method. Sparse kernel category is composed of methods which rely only on a few labelled observations of the training set to assign the label of new observations~\cite{Bishop2006}.

  \Acf{svm}\footnote{\ac{svm} implementation can be found at: \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}} is a sparse kernel method which aims at finding the best linear hyperplane (non-linear separation is discussed further) which separates two classes such that the margin between the two classes is maximized~\cite{Vapnik1963}. \ac{svm} can also be used as a non-linear classifier by performing a kernel trick~\cite{Boser1992}. The original data $\mathbf{x}$ can be projected to a higher-dimension space in which it is assumed that a linear hyperplane will better split the classes. Different kernels are popular such as the \ac{rbf} kernel, polynomial kernels or Gaussian kernel.
In prostate \ac{cad} system, \ac{svm} is the most popular classification method and was used in a multitude of research works~\cite{Artan2009,Artan2010,Chan2003,Kelm2007,Litjens2011,Litjens2012,Liu2013,Lopes2011,Niaf2011,Niaf2012,Ozer2009,Ozer2010,Parfait2012,Peng2013,Sung2011,Tiwari2012,Vos2008,Vos2008a,Vos2010,Vos2012}.

  \Acf{rvm} is a sparse version of Gaussian process previously presented and was proposed by~\cite{Tipping2001}. \ac{rvm} is identical to a Gaussian process with a specific covariance function~\cite{Quinonero-Candela2002}. Ozer et al.~\cite{Ozer2009,Ozer2010} make use of \ac{rvm} and make a comparison with \ac{svm} for the task of \ac{cap} detection.

\item[$-$] \textbf{\textit{Neural network:}} Multilayer perceptron is a feed-forward neural networks considered as the most successful model of this kind in pattern recognition~\cite{Bishop2006}. The most well known model used is based on two layers. Matulewicz et al.~\cite{Matulewicz2013} as well as Parfait et al.~\cite{Parfait2012} used this classifier to classify \ac{mrsi} spectra.
Probabilistic neural networks are another type of feed-forward networks which can be derived from the multilayer perceptron case and was proposed by~\cite{Specht1988}. This classifier can be modelled by changing the activation function of the hidden layer to an exponential function. This method was used  in~\cite{Ampeliotis2007,Ampeliotis2008,Viswanath2011}.

\item[$-$] \textbf{\textit{Graphical model classifiers:}} \ac{mrf} can also be used for classification in order to perform a lesion segmentation method to detect \ac{cap}. Liu et al.~\cite{Liu2009} and Ozer et al.~\cite{Ozer2010} used \ac{mrf} as an unsupervised method to segment lesions in multi-parametric \ac{mri}.
Artan et al.~\cite{Artan2009,Artan2010} used conditional random fields instead of \ac{mrf} for \ac{mri} segmentation.
\end{enumerate}

\subsubsection{Model validation}

In pattern recognition, the validation model for assessing the performance of a classifier plays an important role in the final results. 
Two techniques are broadly used in the development of a \ac{cad} system and are summarized in \ac{tab}~\ref{tab:valmod}.
The most popular technique (see \ac{tab}~\ref{tab:valmod}) is the \acf{loo} technique. From the whole data, one patient is kept for validation and the other cases are used for training. This manipulation is repeated until each patient has been used for validation. This technique is popular when working with a limited number of patients, allowing to train on a representative number of cases even with a small dataset. However, \acf{loo} can suffer from large variance and can be considered as an unreliable estimate~\cite{Efron1983}.

The other technique is the \acf{kcv} technique which is based on splitting the dataset into $k$ subsets where the samples are randomly selected. Then, one fold is kept for the validation and the remaining subsets are used for training. The classification is then repeated as in the \ac{loo} technique. In fact \acf{loo} is a particular case of \acf{kcv} when $k$ equals the number of patients. In the reviewed papers, the typical values used for $k$ were set to three and five. \acf{kcv} is regarded as more appropriate than \acf{loo}, but the number of patients in the dataset needs to be large enough for the results to be meaningful.

\subsubsection{Evaluation measure}\label{subsubsec:eval}

Several metrics can be used in order to assess the performance of a classifier and are summarized in \ac{tab}~\ref{tab:evatec}.
Voxels in the \ac{mri} image are classified into healthy or malign tissue and compared with a ground-truth. This allows to compute a confusion matrix by counting true positive, true negative, false positive and false negative samples. From this analysis, different statistics can be extracted. 

The first statistic used is the accuracy which is computed as the ratio of true detection to the number of samples. However, depending on the strategy employed in the \ac{cad} work-flow, this statistic can be highly biased by a high number of true negative samples which will boost the accuracy score overestimating the actual performance of the classifier.

That is why, the most common statistic computed are sensitivity and specificity which give a full overview of the performance of the classifier. Sensitivity is also called the true positive rate and is equal to the ratio of the true positive samples over the true positive added with the false negative samples as shown in \acs{eq}\,\eqref{eq:sens}. Specificity is also named the true negative rate and is equal to the ratio of the true negative samples over the true negative added with the false positive samples as shown in \acs{eq}\,\eqref{eq:spec}.

\begin{equation}
  SEN = \frac{TP}{TP+FN} \ ,
  \label{eq:sens}
\end{equation}

\begin{equation}
  SPE = \frac{TN}{TN+FP} \ .
  \label{eq:spec}
\end{equation}

These statistics can be used to compute the \acf{roc} curves~\cite{Metz2006}. This analysis represents graphically the sensitivity as a function of (1 - specificity), which is in fact the false positive rate, by varying the discriminative threshold of the classifier. By varying this threshold, more true negative samples will be found but often at the cost of detecting more false negatives. However, this fact is interesting in \ac{cad} since it is possible to obtain a high sensitivity and to ensure that no cancers are missed even if more false alarms have to be investigated. A statistic derived from \ac{roc} analysis is the \acf{auc} which corresponds to the area under the \ac{roc} and is a measure used to make comparisons between models.

The \ac{roc} analysis can be classified as a pixel-based evaluation method. However, a cancer can be also considered as a region. The \acf{froc} extends the \ac{roc} analysis but to a region-based level. The same confusion matrix can be computed were the sample are not pixels but lesions. However, it is important to define what is a true positive sample in that case. Usually, a lesion is considered as a true positive sample if the region detected by the classifier overlaps ``sufficiently'' the one delineated in the ground-truth. However, ``Sufficiently'' is a subjective measure defined by each researcher and can correspond to one pixel only. However, an overlap of 30 to 50 \% is usually adopted.
Finally, in addition to the overlap measure, the Dice's coefficient is often computed to evaluate the accuracy of the lesion localization. This coefficient consists of the ratio between twice the number of pixels in common and the sum of the pixels of the lesions in the ground-truth $GT$ and the output of the classifier $S$, defined as shown in \acs{eq}\,\eqref{eq:dice}.

\begin{equation}
  Q_D = \frac{2 | GT \cap S |}{| GT | + | S |} \ .
  \label{eq:dice}
\end{equation}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../g_lemaitre_state_of_the_art"
%%% End: 