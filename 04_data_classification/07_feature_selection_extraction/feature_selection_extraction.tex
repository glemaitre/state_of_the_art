\subsection{\ac{cadx}: Feature selection and feature extraction} \label{subsec:featureselectionextraction}

As presented in the previous section, a wide variety of features can be computed (see Tab. \ref{tab:feat}). This often leads from multi-parametric \ac{mri} data to a high dimensions feature space which might mislead or corrupt the classifier used for training. Thus, it is often of interest to reduce the number of dimensions before proceeding to the classification task. The strategies used can be grouped as: (i) feature selection and (ii) feature extraction and those methods will be presented in the above sections. However, only the methods used in \ac{cad} system are presented and summarized in Tab. \ref{tab:featext}.

\subsubsection{Feature selection}\label{subsubsec:featsel}

The feature selection strategy is based on selecting the most discriminative feature dimensions of the high-dimensional space. Thus, the low-dimensional space is then composed of a subset of the original features detected. In this section, methods employed in the studies reviewed will be briefly presented. More extensive reviews specific to feature selection can be found in \cite{Saeys2007}.

\cite{Niaf2011,Niaf2012} make use of the p-value by using the independent two-sample t-test with equal mean for each feature dimension. The features can be ranked and the most significant features can be selected. However, this technique suffers from a main drawback since it assumes that each feature is independent, which is unlikely to happen and introduces a high degree of redundancy in the features selected.

\cite{Vos2012} employed a similar feature ranking approach but make use of the Fisher discriminant ratio to compute the relevance of each feature dimension. Once the features are ordered, \cite{Vos2012} select the feature dimensions with the larger Fisher discriminant ratio.

\ac{mi} can also be used to select a subset of feature dimensions. \cite{Peng2005} introduced two main criteria to select the feature dimensions: (i) maximal relevance and (ii) minimum redundancy. Combination of these two criteria is known as \ac{mrmr}\footnote{\ac{mrmr} implementation can be found at: \texttt{http://penglab.janelia.\allowbreak org/proj/mRMR/}} (\cite{Peng2005}) and are computed as a difference or quotient. \cite{Niaf2011,Niaf2012} make use of maximal relevance criterion alone and also of both \ac{mrmr} difference and quotient criterion. \cite{Viswanath2012} also reduced their feature vector via \ac{mrmr} difference and quotient.

\subsubsection{Feature extraction}

The feature extraction strategy is related to dimension reduction methods but not selecting discriminative features. Instead, these methods aim at mapping the data from the high-dimensional space into a low-dimensional space created to maximize the separability between the classes. The mapping can be performed in a linear or a non-linear manner. Again, only methods employed in \ac{cad} system will be reviewed in this section. We refer the reader to the review of \cite{Fodor2002} for a full review of feature extraction techniques.

\ac{pca} is the most commonly used linear mapping method in \ac{cad} prostate (\cite{Jolliffe2002}). \cite{Tiwari2008,Tiwari2009,Tiwari2012} used \ac{pca} in order to reduce the dimensionality of their feature vector.

Non-linear mapping was also used for dimension reduction. It is mainly based on Laplacian eigenmaps and \acf{lle} methods. Laplacian eigenmaps\footnote{Laplacian eigenmap implementation is available at: \texttt{http://www.cse.\allowbreak ohio-state.edu/$\sim$mbelkin/algorithms/algorithms.html}}, also referred as spectral clustering in computer vision, aim to find a low-dimensional space in which the proximity of the data should be preserved from the high-dimensional space (\cite{Shi2000,Belkin2001}). \cite{Tiwari2007,Tiwari2009,Tiwari2009a,Viswanath2008} used this spectral clustering to project their feature vector into a low-dimensional space. The feature space in these studies is usually composed of features extracted from a single or multiple modalities and then concatenated before applying the Laplacian eigenmaps dimension reduction technique.

\cite{Tiwari2009,Tiwari2013} used a slightly different approach by combining the Laplacian eigenmaps techniques with a prior multi-kernel learning strategy.

\ac{lle}\footnote{\ac{lle} implementation is available at: \texttt{http://www.cs.nyu.edu/\allowbreak $\sim$roweis/lle/code.html}} is another common non-linear dimension reduction technique widely used, first proposed by \cite{Roweis2000}. \cite{Tiwari2008} used a modified version of the \ac{lle} algorithm in which they applied \ac{lle} in a bagging approach with multiple neighbourhood sizes. The different embeddings obtained are then fused using the \ac{ml} estimation.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../g_lemaitre_state_of_the_art"
%%% End: 