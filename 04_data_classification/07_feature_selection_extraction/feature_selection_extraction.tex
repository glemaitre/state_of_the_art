\subsection{Feature selection and feature extraction} \label{subsec:featureselectionextraction}

\begin{table}
	\caption{Overview of the feature selection and extraction methods used in \ac{cad} systems.}
	\small
	%\renewcommand{\arraystretch}{1.5}
	\begin{tabular}{p{.65\linewidth} p{.25\linewidth}}
		\hline \\ [-1.5ex]
		\textbf{Dimension reduction methods} & \textbf{References} \\ \\ [-1.5ex]
		\hline \\ [-1.5ex]
		\textit{Feature selection:} & \\ \\ [-1.5ex]
		\quad Statistical test & $[$17-18,41$]$ \\
		\quad \ac{mi}-based methods & $[$18-19,37$]$ \\ \\ [-1.5ex]
		\textit{Feature extraction:} & \\ \\ [-1.5ex]
		\quad Linear mapping & \\
		\quad \quad \acs{pca} & $[$27-28,31$]$ \\
		\quad Non-linear mapping & \\
		\quad \quad Laplacian eigenmaps & $[$26,28-30,33,36$]$ \\
		\quad \quad \acs{lle} and \acs{lle}-based & $[$27-28,33-34$]$ \\ \\ [-1.5ex]
		\hline
	\end{tabular}
	\label{tab:featext}
\end{table}

As presented in the previous section, a wide variety of features can be computed (see Tab. \ref{tab:feat}). Using multi-parametric data as well as multiple features lead to a high complexity feature space which might mislead or corrupt the classifier which have to be trained. Thus, one will be interesting to reduce the number of dimension of the feature space before to proceed to any classification task. The strategy used can be group into two groups: (i) feature selection and (ii) feature extraction. The methods used for \ac{cad} system are summarized in Tab. \ref{tab:featext}.

\subsubsection{Feature selection}\label{subsubsec:featsel}

The feature selection strategy corresponds in selecting the most discriminative feature dimension of the high-dimensional space. Thus, the low-dimensional space is then composed of a subset of the original features detected. In this section, methods employed in the studies reviewed will be briefly presented. More extensive review specific to feature selection can be found in \cite{Saeys2007}.

\cite{Niaf2011,Niaf2012} make use of the \textit{p-value} by using the independent two-sample \textit{t-test} with equal mean for each feature dimension. In this statistical test, it is assumed two classes, \ac{cap} against healthy. Hence, for each particular feature, the distribution of each class can be characterized by their means $\bar{X}_1$ and $\bar{X}_2$ and standard deviation $s_{X_1}$ and $s_{X_2}$, respectively. Hence, the null hypothesis tested is based on the fact that these both distribution means are equal. Thus, the t-statistic related to verify the null hypothesis is then formalized such that:

\begin{eqnarray}
t & = & \frac{\bar {X}_1 - \bar{X}_2}{s_{X_1X_2} \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} \ , \label{eq:tstat} \\
s_{X_1X_2} & = & \sqrt{\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}} \ . \nonumber
\end{eqnarray}

\noindent where $n_1$ and $n_2$ are the number of sample in each class.

From Eq. \ref{eq:tstat}, it can be seen that the more the means of the class distribution diverge, larger the $t$-statistic $t$ will be implying that this particular feature is more relevant and able to make the distinction between the two classes to separate. 

The $p$-value statistic can be deduced from the $t$-test and corresponds to the probability to obtain such an extreme test assuming that the null hypothesis is true \cite{Goodman1999}. Hence, smaller is the $p$-value, more likely is to reject the null hypothesis and more relevant the feature is likely to be.

Finally, the feature can be ranked and the most significant features can be selected, by defining the number of features wanted. However, this technique suffers from a main drawback since that it is assumed that each feature are independent which is unlikely to happen and introduce a high redundancy in the feature selected.

\cite{Vos2012} employed a similar feature ranking approach but make use of the Fisher discriminant ratio to compute the relevance of each feature dimension. Taking the aforementioned formulation, the Fisher discriminant ratio is formalized as the ratio of the variance between classes to the variance within classes such that:

\begin{equation}
F_r = \frac{\bar{X}_1 - \bar{X}_2}{s^{2}_{X_1}+s^{2}_{X_2}} \ .
\label{eq:fisherratio}
\end{equation}

Hence, a feature dimension can be seen as more relevant when the variance between classes is maximum and the variance within classes in minimum. Once the features ordered, \cite{Vos2012} select the feature dimensions with the larger Fisher discriminant ratio.

\ac{mi} can also be used to select the subset of feature dimension. Definition of the \ac{mi} was presented in Sect. \ref{subsubsec:simmea} and formalized in \ref{eq:midef}. The computation of the entropies involves the estimation of some \acp{pdf} and the data being usually continuous variables, it is then necessary to estimate the \acp{pdf} using method such as Parzen windows.

\cite{Peng2005} introduced two main criteria to select the features dimensions and then combined: (i) maximal relevance and (ii) minimum redundancy.

Maximal relevance criterion is based on the paradigm that the classes and the feature dimension which have to be selected have to share a maximal \ac{mi} and can be formalized such that:

\begin{eqnarray}
	&&\argmax Rel(\mathbf{x},c) \ , \nonumber \\
	&& Rel(\mathbf{x},c) = \frac{1}{|\mathbf{x}|} \sum_{x_i \in \mathbf{x}} MI(x_i,c)  \ . \label{eq:mRel}
\end{eqnarray}

\noindent where $\mathbf{x} = \{x_i,i=1,\cdots,d\}$ is a feature vector of $d$ dimensions, $c$ is the class considered and $MI(.)$ is the \ac{mi}.

As in the previous method, using maximal relevance criterion alone will imply the independence between each feature dimension which is usually not true.

Minimal redundancy criterion will force to select a new feature dimension which shares as less as possible \ac{mi} with previously selected feature dimension. It can be formalized as:

\begin{eqnarray}
	&&\argmin Red(\mathbf{x}) \ , \nonumber \\
	&& Red(\mathbf{x}) = \frac{1}{|\mathbf{x}|^2} \sum_{x_i,x_j \in \mathbf{x}} MI(x_i,x_j)  \ . \label{eq:mRed}
\end{eqnarray}

Combination of these two previous criteria is known as \ac{mrmr}\footnote{\ac{mrmr} implementation can be found at: \texttt{http://penglab.janelia.\allowbreak org/proj/mRMR/}} (\cite{Peng2005}) and can be computed such as a difference or a quotient of the Eq. \ref{eq:mRel} and \ref{eq:mRed}.

\cite{Niaf2011,Niaf2012} make use of maximal relevance criterion alone and also of both \ac{mrmr} difference and quotient criterion. \cite{Viswanath2012} also reduced their feature vector via \ac{mrmr} difference and quotient.

\subsubsection{Feature extraction}

The feature extraction strategy is related to dimension reduction methods and are not selecting discriminative features. Instead, these methods aimed at mapping the data from the high-dimensional space into a low-dimensional space created to maximize the separability between the classes. The mapping can be performed from a linear or a non-linear manner. Only methods employed in \ac{cad} system will be reviewed in this section. We guide the reader to the review of \cite{Fodor2002} for a full review of feature extraction techniques.

Linear mapping method used to reduce the dimensionality in \ac{cad} system is usually the \ac{pca}. 

\ac{pca} is a method allowing to find the orthogonal linear transform mapping the original data into a low-dimensional space. The space is defined such that the linear combinations of the original data with the $k^{th}$ greatest variances will lie on the $k^{th}$ principal components (\cite{Jolliffe2002}).

The principal components can then by computing using the eigenvectors-eigenvalues decomposition on the covariance matrix. Let's define the $\mathbf{x}$ being the data matrix. Then the covariance matrix is defined such as:

\begin{equation}
	\Sigma = \mathbf{x}^{\text{T}} \mathbf{x} \ .
	\label{eq:covmat}
\end{equation}

The eigenvectors-eigenvalues decomposition can be formalized such as:

\begin{equation}
	\mathbf{v}^{-1} \Sigma \mathbf{v} = \Lambda \ .
	\label{eq:eigpca}
\end{equation}

\noindent where $\mathbf{v}$ is the eigenvectors matrix and $\Lambda$ is a diagonal matrix containing the eigenvalues. 

It is then possible to find the new low-dimensional space by sorting the eigenvectors using the eigenvalues and finally select the largest eigenvalues. The total variation being equal to the sum of the eigenvalues of the covariance matrix (\cite{Fodor2002}), usually the number of principal components correspond to the $95\%$ to $98\%$ of the cumulative sum of the eigenvalues. \cite{Tiwari2008,Tiwari2009,Tiwari2012} used \ac{pca} in order to reduce the dimensionality of their feature vector.

Non-linear mapping was also used for dimension reduction and mainly based on Laplacian eigenmaps and \acf{lle} methods.

Laplacian eigenmaps\footnote{Laplacian eigenmap implementation is available at: \texttt{http://www.cse.\allowbreak ohio-state.edu/~mbelkin/algorithms/algorithms.html}} or also referred as spectral clustering in computer vision aimed to find a low-dimensional space in which the proximity of the data should be preserved from the high-dimensional space (\cite{Shi2000,Belkin2001}). Thus, two adjacent data points in the high-dimensional space should also be close in the low-dimensional space. In like manner, two far away data points in the high-dimensional space should be also distant in the low-dimensional space. To compute this projection, an adjacency matrix is defined such as:

\begin{equation}
	W(i,j) = \exp \| x_i - x_j \|_2 \ .
	\label{eq:gew}
\end{equation}

Then, the low-dimensional space will be found by solving the generalized eigenvectors-eigenvalues problem such as:

\begin{equation}
	(D-W)\mathbf{y} = \lambda D \mathbf{y} \ .
	\label{eq:geeig}
\end{equation}

\noindent where $D$ i a diagonal matrix such that $D(i,i) = \sum_j W(j,i)$.

Finally the low-dimensional space is defined by the $k$ eigenvectors of the $k$ smallest eigenvalues (\cite{Belkin2001}). \cite{Tiwari2007,Tiwari2009,Tiwari2009a,Viswanath2008} used this spectral clustering to project their feature vector into a low-dimensional space. The feature space in these studies is usually composed of features extracted from a single or multiple modalities and then concatenated before applying the Laplacian eigenmaps dimension reduction technique.

\cite{Tiwari2009,Tiwari2013} used a slightly different approach by combining the Laplacian eigenmaps techniques with a prior multi-kernel learning strategy. First, multiple feature were extracted for multiple modalities. The features of a single modalities were then mapped to an higher dimensional space via the Kernel trick (\cite{Aizerman1964}) and more precisely using a Gaussian kernel. Then, each kernel associated with each modality are linearly combined to obtain a combined kernel $K$. Then, the computation of the adjacency matrix $W$ takes place and the same scheme as in Laplacian eigenmaps is performed. However, in order to used the combined kernel, Eq. \ref{eq:geeig} becomes as shown in Eq. \ref{eq:sesmik} and can be solved as a generalized eigenvectors-eigenvalues problem as previously.

\begin{equation}
	K (D-W) K^{\text{T}} \mathbf{y} = \lambda K D K^{\text{T}} \mathbf{y} \ .
	\label{eq:sesmik}
\end{equation}

\cite{Viswanath2011} used Laplacian eigenmaps inside a bagging framework in which multiple embeddings are generated by successively selecting feature dimensions.

\ac{lle}\footnote{\ac{lle} implementation is available at: \texttt{http://www.cs.nyu.edu/\allowbreak ~roweis/lle/code.html}} is another non-linear dimension technique broadly known firstly proposed by \cite{Roweis2000}. \ac{lle} is based on the fact that a data point in the feature space can be characterized by its neighbours. Thus, it was proposed to represent each data point in the high-dimensional space as the linear combination of its $k$-nearest neighbours. This can be expressed such as:

\begin{equation}
	\hat{\mathbf{x}}_i = \sum_j W(i,j) \mathbf{x}_j \ .
	\label{eq:lincomlle}
\end{equation}

\noindent where $\mathbf{x}_i$ and $\mathbf{x}_j$ are the data point considered and its neighbours data points, respectively.

Hence, this problem which have to be solved at this stage is to estimate the weight matrix $W$. This problem can be tackled using a least square optimization scheme by optimizing the following objective function:

\begin{eqnarray}
	\hat{W} & = & \argmin_{W} \sum_i | \mathbf{x}_i - \sum_j W(i,j)\mathbf{x}_j |^{2} \ , \label{eq:lslle} \\
	&& \text{subject to } \sum_j W(i,j) = 1 \ , \nonumber
\end{eqnarray}

Then, the essence of \ac{lle} is to project the data into a low-dimensional keeping the data organization. Thus, the projection into the low-dimensional space can be seen as an optimization problem such that:

\begin{equation}
	\hat{\mathbf{y}} = \argmin_{\mathbf{y}} \sum_i | \mathbf{y}_i - \sum_j W(i,j)\mathbf{y}_j |^{2} \ .
	\label{eq:lowprojlle}
\end{equation}

This optimization can be performed as an eigenvectors-eigenvalues problem by finding the $k^{\text{th}}$ eigenvectors corresponding the $k^{\text{th}}$ smallest eigenvalues of the sparse matrix $(I-W)^{\text{T}}(I-W)$. \cite{Tiwari2008,Tiwari2009,Viswanath2008,Viswanath2008a} used \ac{lle} as dimension reduction technique to reduce the complexity of their feature vector.

\cite{Tiwari2008} used a modified version of the \ac{lle} algorithm in which they applied \ac{lle} in a bagging approach with multiple size of neighbourhood. The embedding obtained are then fusion using the maximum likelihood estimation.